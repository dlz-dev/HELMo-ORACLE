# ğŸ—ºï¸ Roadmap : Du Prototype Oracle au Moteur RAG Entreprise

~~## Phase 1 : Ingestion "Context-Aware" (Markdown & Documents)~~

**Le ProblÃ¨me :** Actuellement, le convertisseur Markdown dÃ©coupe le fichier ligne par ligne ou par bloc isolÃ©s. Si l'Oracle trouve une rÃ©ponse dans une liste Ã  puces, il perd le titre qui expliquait de quoi parlait cette liste.

**La Solution (InspirÃ©e de LangChain & Medium) :**

* **MarkdownHeaderTextSplitter :** Au lieu de dÃ©couper par taille, on dÃ©coupe par structure. Le texte est rattachÃ© Ã  ses mÃ©tadonnÃ©es (Headers). Si le bot trouve un paragraphe, il "sait" qu'il appartient au chapitre "RÃ¨glement IntÃ©rieur" grÃ¢ce aux mÃ©tadonnÃ©es injectÃ©es.
* **Metadata Enrichment :** Chaque vecteur dans Supabase doit contenir le nom du fichier, la page (pour les PDF) et le niveau de titre. Cela permet Ã  l'IA de citer ses sources prÃ©cisÃ©ment ("Selon le document RH, page 12...").
  * Chaque "chunk" (morceau de texte) envoyÃ© Ã  Supabase doit Ãªtre accompagnÃ© d'un dictionnaire de mÃ©tadonnÃ©es (titre du document, auteur, date, etc.).

**Sources :** 
- https://medium.com/@vishalkhushlani123/building-a-markdown-knowledge-ingestor-for-rag-with-langchain-ba201515f6c4
- https://medium.com/@msriarunm/document-loaders-feeding-data-into-rag-91e3ff36ff60

---

## Phase 2 : Late Chunking (Le "Copier/Coller" LocalGPT)

**Le ProblÃ¨me :** Dans les textes longs, un "chunk" au milieu du document perd le contexte global. Le vecteur reprÃ©sente le sens du paragraphe, mais oublie qu'il fait partie d'un contrat spÃ©cifique signÃ© en 2024.

**La Solution (ImplÃ©mentation LocalGPT) :**

* **Global Encoding :** On passe le document entier (ou de trÃ¨s grandes sections) dans le modÃ¨le d'embedding avant de dÃ©couper.
* **Contextual Embeddings :** Le vecteur gÃ©nÃ©rÃ© pour un paragraphe "hÃ©rite" du sens global du document. C'est crucial pour les entreprises traitant des rapports de 50 pages oÃ¹ le sujet n'est rappelÃ© qu'en introduction.

**Sources :** 
- https://jina.ai/news/late-chunking-in-long-context-embedding-models/
- https://github.com/PromtEngineer/localGPT?tab=readme-ov-file (NumÃ©ro 2 des tendances GitHub sur un RAG Local)

---

## Phase 3 : Intelligence & MÃ©moire (ExpÃ©rience Utilisateur)

**Le ProblÃ¨me :** L'historique Streamlit actuel est volatil et s'efface au rafraÃ®chissement, et l'IA cherche parfois dans les documents pour rien.

**La Solution :**

* **Local Persistent History :** Sauvegarde des Ã©changes dans un dossier `storage/sessions/` sous format JSON. Chaque utilisateur retrouve sa conversation grÃ¢ce Ã  un ID de session.
* **ConversationSummaryBufferMemory :** Utilisation de LangChain pour rÃ©sumer les vieux Ã©changes tout en gardant les messages rÃ©cents intacts, Ã©vitant de saturer la fenÃªtre de contexte.
* **Query Triage :** Ajout d'une Ã©tape de dÃ©cision oÃ¹ l'IA classifie la question : "Recherche RAG requise", "RÃ©ponse via historique" ou "Simple politesse".

---

## Phase 4 : Recherche Hybride & Re-ranking

**Le ProblÃ¨me :** La recherche vectorielle (sÃ©mantique) est excellente pour le sens, mais mÃ©diocre pour les termes techniques exacts ou les codes produits (ex: "Project-X92"). Ton `VectorManager` actuel ne fait que de la distance Euclidienne.

**La Solution (Architecture Pro) :**

* **BM25 + Vector :** On effectue deux recherches en parallÃ¨le. Une recherche par mots-clÃ©s "classique" (FTS - Full Text Search dans Supabase) et une recherche vectorielle.
* **Reranker :** On prend les 10 meilleurs rÃ©sultats des deux mÃ©thodes et on utilise un modÃ¨le de "Re-ranking" (plus petit et rapide que Llama) pour classer ces rÃ©sultats par pertinence rÃ©elle avant de les donner Ã  l'Oracle (via ColBERT).

**Source :**
- https://github.com/PromtEngineer/localGPT?tab=readme-ov-file (NumÃ©ro 2 des tendances GitHub sur un RAG Local)

---

## Phase 5 : FiabilitÃ© & SÃ©curitÃ© (Production)

**Le ProblÃ¨me :** Envoyer des donnÃ©es brutes en entreprise pose des risques de fuites de donnÃ©es sensibles et de bugs en production.

**La Solution :**

* **Automated Testing :** Mise en place de tests unitaires (Pytest) pour valider que les convertisseurs (CSV, MD, PDF) ne corrompent pas les donnÃ©es lors de l'ingestion.
* **PII Masking (Anonymisation) :** Filtre de sÃ©curitÃ© dÃ©tectant les noms, emails ou numÃ©ros de tÃ©lÃ©phone pour les masquer avant l'envoi aux APIs cloud (Groq/OpenAI).
* **Local Switch :** Option dans `config.yaml` pour basculer sur un modÃ¨le local (Ollama/Llama 3) pour les documents classÃ©s confidentiels.

---

## Phase 6 : Interface & DÃ©ploiement (ScalabilitÃ©)

**Le ProblÃ¨me :** Le projet doit pouvoir Ãªtre utilisÃ© par n'est pas limitÃ© Ã  Streamlit et doit accepter des nouveaux documents facilement.

**La Solution :**

* **Drag & Drop UI :** IntÃ©gration d'un module d'upload direct dans Streamlit pour alimenter la base de connaissances sans relancer de scripts manuels.
* **Architecture Multi-plateforme :** SÃ©paration du code en deux parties : un **Backend (FastAPI)** qui gÃ¨re l'IA et un **Frontend (Streamlit/React)**. Cela permet d'intÃ©grer l'Oracle dans Slack, Teams ou un site web mÃ©tier.

---

## ğŸ› ï¸ Zoom technique : Le Switch Local vs API

Pour implÃ©menter ce que tu as en tÃªte, la modification se ferait dans la barre latÃ©rale de ton application :

> **Interface :** Un bouton radio `st.sidebar.radio("Mode d'intelligence", ["Cloud (Groq)", "Local (Ollama)"])`. <br>
> **Logique :** Si "Local" est choisi, le code instancie `ChatOllama(model="llama3")` au lieu de `ChatGroq`. Cela permet Ã  une entreprise de traiter des documents ultra-confidentiels sans jamais utiliser internet.

**Souhaites-tu que je te prÃ©pare le code du sÃ©lecteur (Phase 4) pour l'intÃ©grer dans ta barre latÃ©rale Streamlit ?** ğŸ”®